---
layout: page  
title: "Neural Network - math & visualizations"
date: 2024-12-10  
categories: [Python]  
tags: [Machine Learning, Neural Networks, Python, Backpropagation, Math, Visualizations, Manim]  
---

---

## Repository
The complete implementation and additional documentation are available on [GitHub](https://github.com/sarawaniolka/Neural_Networks_Math).

## Description
This project showcases my journey in creating a neural network from scratch, purely relying on mathematical principles and fundamental Python programming, without additional libraries. The goal was to understand neural networks' inner workings deeply, without leveraging external frameworks like TensorFlow or PyTorch.
The second part included using Manim Community to visualize how neural networks work.


## The architecture
<video width="600" controls>
  <source src="https://github.com/sarawaniolka/Neural_Networks_Math/blob/master/media/videos/manim_visualizations/1080p60/ArchitectureVisualization.mp4?raw=true" type="video/mp4">
  Your browser does not support the video tag.
</video>
The architecture visualization showcases the structure of a neural network with four layers, where the input layer consists of 3 neurons, the first hidden layer has 5 neurons, the second hidden layer contains 4 neurons, and the output layer has 2 neurons. Each layer is represented as a group of neurons, with connections between adjacent layers forming a network. The number of neurons in each layer is visually indicated, and the layers are color-coded for clarity

## The forward pass
<video width="600" controls>
  <source src="https://github.com/sarawaniolka/Neural_Networks_Math/blob/master/media/videos/manim_visualizations/1080p60/ForwardPassVisualization.mp4?raw=true" type="video/mp4">
  Your browser does not support the video tag.
</video>
The forward pass visualization illustrates the flow of data through a neural network with 3 layers. Starting from the input layer with 3 neurons, the data is multiplied by weights and passed through the first hidden layer with 5 neurons using the ReLU activation function. The results from this layer are then passed to the second hidden layer with 4 neurons, where a similar forward pass occurs. Finally, the data reaches the output layer with 2 neurons, where the softmax activation function is applied. The calculations, including weighted sums and activation functions, are shown step-by-step next to each neuron, illustrating how the network processes the input data to produce the final output.



## Network Components  
### Core Functions  
- **Forward Propagation**: Computes predictions step-by-step through layers.  
- **Backpropagation**: Derives gradients using chain rule principles.  
- **Optimization**: Adjusts weights and biases based on gradients and a learning rate.  

### Features  
- **Custom Layer Initialization**: Randomly initializes weights and biases.  
- **Evaluation**: Computes accuracy and loss for model validation.  


## Tools and Technologies  
- **Programming Language**: Python  
- **Libraries**:  
  - `math` and `random`: For core computations.  
  - `numpy`: For efficient array operations like `argmax`.  
  - `scikit-learn`: For dataset handling, scaling, and splitting.  - used only to test the nn ans use a built-in dataset


## Results  
The implemented neural network achieved competitive performance on the Iris dataset.  

- **Architecture**: 4 input neurons → 8 neurons in the first hidden layer → 6 neurons in the second hidden layer → 3 output neurons.  
- **Testing Accuracy**: Over 96% after 100 epochs.  


