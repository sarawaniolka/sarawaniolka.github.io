---
layout: page  
title: "Designing a Neural Network From Scratch Using Only Math"  
date: 2024-12-10  
categories: [Python]  
tags: [Machine Learning, Neural Networks, Python, Backpropagation, Math]  
---

---

## Repository
The complete implementation and additional documentation are available on [GitHub](https://github.com/sarawaniolka/Neural_Networks_Math).


## Description
This project showcases my journey in creating a neural network from scratch, purely relying on mathematical principles and fundamental Python programming, without additional libraries. The goal was to understand neural networks' inner workings deeply, without leveraging external frameworks like TensorFlow or PyTorch.

## Introduction  
The project explores the mathematical foundations of neural networks, emphasizing manual implementation. By stripping down to basics, I aimed to strengthen my understanding of core concepts such as forward propagation, backpropagation, activation functions, and optimization techniques.

## Architecture and Implementation  
The neural network was implemented to handle tasks such as classification on the Iris dataset. Key components included:  

- **Layer Function**: Computes the linear transformation \( Z = WX + b \).  
- **Activation Functions**:  
  - **ReLU**: Applies the rectified linear unit function for non-linearity.  
  - **Softmax**: Converts logits into probabilities for classification.  
- **Loss Functions**:  
  - **Mean Squared Error (MSE)**: Measures prediction error.  
- **Backpropagation**: Implements gradient computation for updating weights and biases.  
- **Training Loop**: Iteratively improves the model over epochs.

## Network Components  
### Core Functions  
- **Forward Propagation**: Computes predictions step-by-step through layers.  
- **Backpropagation**: Derives gradients using chain rule principles.  
- **Optimization**: Adjusts weights and biases based on gradients and a learning rate.  

### Features  
- **Custom Layer Initialization**: Randomly initializes weights and biases.  
- **Evaluation**: Computes accuracy and loss for model validation.  


## Tools and Technologies  
- **Programming Language**: Python  
- **Libraries**:  
  - `math` and `random`: For core computations.  
  - `numpy`: For efficient array operations like `argmax`.  
  - `scikit-learn`: For dataset handling, scaling, and splitting.  - used only to test the nn ans use a built-in dataset


## Results  
The implemented neural network achieved competitive performance on the Iris dataset.  

- **Architecture**: 4 input neurons → 8 neurons in the first hidden layer → 6 neurons in the second hidden layer → 3 output neurons.  
- **Testing Accuracy**: Over 96% after 100 epochs.  


